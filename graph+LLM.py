# çŸ¥è¯†å›¾è°±ä»£ç ï¼ˆæ™ºèƒ½æ›´æ–°çš„ç²¾ç®€ç‰ˆï¼‰
import json
import ast
from neo4j import GraphDatabase
from openai import OpenAI
import time
from typing import List, Dict
import requests
import os


class ContentFocusedKG:
    def __init__(self, uri, username, password, openai_api_key=None, tongyi_api_key=None):
        self.driver = GraphDatabase.driver(uri, auth=(username, password))

        # è®¾ç½®OpenAI API
        if openai_api_key:
            self.client = OpenAI(
                api_key=openai_api_key,
                base_url="https://api.chatanywhere.tech/v1"
            )
        else:
            self.client = None

        # è®¾ç½®é€šä¹‰åƒé—®API
        self.tongyi_api_key = tongyi_api_key

    def close(self):
        self.driver.close()

    def execute_query(self, query, parameters=None):
        with self.driver.session() as session:
            return session.run(query, parameters).data()

    def clear_database(self):
        with self.driver.session() as session:
            session.run("MATCH (n) DETACH DELETE n")

    def parse_dialog(self, dialog_str):
        """è§£æå¯¹è¯å­—ç¬¦ä¸²ï¼Œæ”¯æŒå¤šç§æ ¼å¼"""
        if not dialog_str or dialog_str == "[]":
            return []

        try:
            # å°è¯•JSONæ ¼å¼
            parsed = json.loads(dialog_str)
            return parsed if isinstance(parsed, list) else []
        except json.JSONDecodeError:
            try:
                # å°è¯•Pythonå­—é¢é‡æ ¼å¼ï¼ˆå¤„ç†å•å¼•å·çš„æƒ…å†µï¼‰
                parsed = ast.literal_eval(dialog_str)
                return parsed if isinstance(parsed, list) else []
            except (ValueError, SyntaxError):
                return []

    def summarize_content_with_dialogs(self, content: str, dialog_messages: List[Dict]) -> str:
        """ä½¿ç”¨LLMæ€»ç»“Contentå’Œæ‰€æœ‰Dialogçš„ç»¼åˆä¿¡æ¯ï¼Œç”Ÿæˆ100å­—æ€»ç»“"""
        # æ„å»ºå®Œæ•´çš„å†…å®¹å’Œå¯¹è¯æ–‡æœ¬
        full_text = f"ç¬”è®°å†…å®¹: {content}\n\n"

        if dialog_messages:
            full_text += "ç›¸å…³å¯¹è¯:\n"
            for i, msg in enumerate(dialog_messages, 1):
                role = msg.get('role', 'user')
                msg_content = msg.get('content', '')
                role_name = "ç”¨æˆ·" if role == 'user' else "AIåŠ©æ‰‹"
                full_text += f"{role_name}: {msg_content}\n"

        # è°ƒç”¨OpenAI APIè¿›è¡Œç»¼åˆæ€»ç»“
        response = self.client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {
                    "role": "system",
                    "content": "è¯·å¯¹ä»¥ä¸‹ç¬”è®°å†…å®¹å’Œç›¸å…³å¯¹è¯è¿›è¡Œç»¼åˆæ€»ç»“ï¼Œé‡ç‚¹æ¦‚æ‹¬æ ¸å¿ƒä¸»é¢˜ã€å…³é”®è§‚ç‚¹å’Œè®¨è®ºè¦ç‚¹ã€‚æ€»ç»“åº”è¯¥æ˜¯ä¸­æ–‡ï¼Œæ§åˆ¶åœ¨100å­—ä»¥å†…ï¼Œè¦å®Œæ•´ä½“ç°å†…å®¹çš„æ ¸å¿ƒä»·å€¼ã€‚"
                },
                {
                    "role": "user",
                    "content": full_text
                }
            ],
            max_tokens=150,
            temperature=0.3
        )

        summary = response.choices[0].message.content.strip()
        time.sleep(0.1)
        return summary

    def calculate_tongyi_similarity(self, text1: str, text2: str, max_retries: int = 3) -> float:
        """ä½¿ç”¨é€šä¹‰åƒé—®HTTP APIè®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
        url = "https://dashscope.aliyuncs.com/api/v1/services/rerank/text-rerank/text-rerank"
        headers = {
            "Authorization": f"Bearer {self.tongyi_api_key}",
            "Content-Type": "application/json"
        }

        data = {
            "model": "gte-rerank",
            "input": {
                "query": text1,
                "documents": [text2]
            },
            "parameters": {
                "return_documents": True,
                "top_n": 1
            }
        }

        response = requests.post(url, headers=headers, json=data, timeout=30)
        result = response.json()

        results = result["output"]["results"]
        similarity = float(results[0]["relevance_score"])
        return max(0.0, min(1.0, similarity))

    def calculate_all_nodes_similarity_with_tongyi(self, session):
        """ä½¿ç”¨é€šä¹‰åƒé—®rerank APIè®¡ç®—æ‰€æœ‰èŠ‚ç‚¹ä¹‹é—´çš„ç›¸ä¼¼åº¦"""
        content_nodes = self.execute_query("""
            MATCH (c:Content)
            RETURN c.id as id, c.summary as content, 'Content' as node_type
        """)

        all_nodes = content_nodes

        if len(all_nodes) < 2:
            return

        valid_nodes = []
        for node in all_nodes:
            content_text = node.get('content', '').strip()
            if content_text:
                valid_nodes.append({
                    'id': node.get('id'),
                    'type': node.get('node_type'),
                    'content': content_text
                })

        if len(valid_nodes) < 2:
            return

        similarity_count = 0

        for i in range(len(valid_nodes)):
            for j in range(i + 1, len(valid_nodes)):
                node1 = valid_nodes[i]
                node2 = valid_nodes[j]

                similarity_score = self.calculate_tongyi_similarity(
                    node1['content'],
                    node2['content']
                )

                if similarity_score > 0.5:
                    distance_weight = 1.0 - similarity_score

                    self.create_similarity_relationship(
                        session, node1, node2, similarity_score, distance_weight
                    )
                    similarity_count += 1

                # å¢åŠ å»¶è¿Ÿä»¥é¿å…APIé™æµ
                time.sleep(0.2)

    def create_similarity_relationship(self, session, node1, node2, similarity_score, distance_weight):
        """æ ¹æ®èŠ‚ç‚¹ç±»å‹åˆ›å»ºç›¸ä¼¼æ€§å…³ç³»"""
        node1_id = node1['id']
        node2_id = node2['id']

        # ç°åœ¨åªæœ‰Content to Contentå…³ç³»
        query = """
            MATCH (n1:Content {id: $node1_id}), (n2:Content {id: $node2_id})
            MERGE (n1)-[:SIMILAR_CONTENT {
                similarity: $similarity,
                weight: $weight,
                distance: $distance,
                type: 'content_to_content',
                method: 'tongyi_rerank'
            }]-(n2)
        """

        session.run(query,
                    node1_id=node1_id,
                    node2_id=node2_id,
                    similarity=similarity_score,
                    weight=distance_weight,
                    distance=distance_weight)

    def update_graph(self, json_file_path, force_full_rebuild=False):
        """æ™ºèƒ½æ›´æ–°çŸ¥è¯†å›¾è°±ï¼ˆè‡ªåŠ¨åˆ¤æ–­å…¨é‡å¯¼å…¥æˆ–å¢é‡æ›´æ–°ï¼‰"""

        # è¯»å–æ–°æ•°æ®
        with open(json_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        new_notes = data.get('output', [])

        if not new_notes:
            print("ğŸ“­ æ²¡æœ‰æ–°å†…å®¹éœ€è¦æ›´æ–°")
            return False  # âœ… æ˜ç¡®è¿”å› False

        # æ£€æŸ¥æ•°æ®åº“æ˜¯å¦ä¸ºç©ºæˆ–å¼ºåˆ¶é‡å»º
        existing_count = self.execute_query("MATCH (c:Content) RETURN count(c) as count")[0]['count']
        is_database_empty = existing_count == 0

        if is_database_empty or force_full_rebuild:
            # å…¨é‡å¯¼å…¥æ¨¡å¼
            print(f"ğŸ”„ æ‰§è¡Œå…¨é‡å¯¼å…¥ - æ•°æ®åº“{'ä¸ºç©º' if is_database_empty else 'å¼ºåˆ¶é‡å»º'}")

            if not is_database_empty:
                self.clear_database()

            with self.driver.session() as session:
                for note in new_notes:
                    self.process_content_node(session, note)
                print(f"âœ… åˆ›å»ºäº† {len(new_notes)} ä¸ªèŠ‚ç‚¹")
                self.calculate_all_nodes_similarity_with_tongyi(session)

            print("âœ… å…¨é‡å¯¼å…¥å®Œæˆï¼")
            return True  # âœ… æ·»åŠ è¿”å›å€¼
        else:
            # å¢é‡æ›´æ–°æ¨¡å¼
            print("ğŸ”„ æ‰§è¡Œå¢é‡æ›´æ–°")

            # è·å–æ•°æ®åº“ä¸­å·²å­˜åœ¨çš„Content ID
            existing_ids = set()
            existing_content_query = self.execute_query("MATCH (c:Content) RETURN c.id as id")
            for item in existing_content_query:
                existing_ids.add(item['id'])

            # åªå¤„ç†æ–°å¢æ•°æ®ï¼Œè·³è¿‡å·²å­˜åœ¨çš„æ•°æ®
            new_notes_to_add = []
            skipped_count = 0

            for note in new_notes:
                note_id = note.get('note_id')
                if note_id in existing_ids:
                    skipped_count += 1
                else:
                    new_notes_to_add.append(note)

            print(f"ğŸ†• æ–°å¢: {len(new_notes_to_add)} æ¡ï¼Œâ­ï¸ è·³è¿‡å·²å­˜åœ¨: {skipped_count} æ¡")

            if not new_notes_to_add:
                print("ğŸ“­ æ²¡æœ‰æ–°å†…å®¹éœ€è¦æ·»åŠ ")
                print("âœ… å¢é‡æ›´æ–°å®Œæˆï¼")
                return False  # âœ… æ˜ç¡®è¿”å› False

            # åªåˆ›å»ºæ–°èŠ‚ç‚¹
            with self.driver.session() as session:
                for note in new_notes_to_add:
                    self.process_content_node(session, note)

            # åªä¸ºæ–°å¢èŠ‚ç‚¹è®¡ç®—ç›¸ä¼¼åº¦
            print("âš¡ å¼€å§‹ä¸ºæ–°å¢èŠ‚ç‚¹è®¡ç®—ç›¸ä¼¼åº¦...")

            # è·å–æ‰€æœ‰èŠ‚ç‚¹ï¼ˆåŒ…æ‹¬æ–°å¢çš„ï¼‰
            all_nodes = self.execute_query("""
                MATCH (c:Content)
                RETURN c.id as id, c.summary as content
            """)

            # åˆ†ç±»èŠ‚ç‚¹
            new_node_ids = set(note.get('note_id') for note in new_notes_to_add)
            new_nodes = []
            existing_nodes = []

            for node in all_nodes:
                content_text = node.get('content', '').strip()
                if content_text:
                    node_info = {
                        'id': node.get('id'),
                        'type': 'Content',
                        'content': content_text
                    }

                    if node.get('id') in new_node_ids:
                        new_nodes.append(node_info)
                    else:
                        existing_nodes.append(node_info)

            similarity_count = 0

            with self.driver.session() as similarity_session:
                # è®¡ç®—æ–°å¢èŠ‚ç‚¹ä¸å·²å­˜åœ¨èŠ‚ç‚¹çš„ç›¸ä¼¼åº¦
                for new_node in new_nodes:
                    for existing_node in existing_nodes:
                        similarity_score = self.calculate_tongyi_similarity(
                            new_node['content'], existing_node['content']
                        )

                        if similarity_score > 0.5:
                            distance_weight = 1.0 - similarity_score
                            self.create_similarity_relationship(
                                similarity_session, new_node, existing_node, similarity_score, distance_weight
                            )
                            similarity_count += 1

                        time.sleep(0.2)

                # è®¡ç®—æ–°å¢èŠ‚ç‚¹ä¹‹é—´çš„ç›¸ä¼¼åº¦
                for i in range(len(new_nodes)):
                    for j in range(i + 1, len(new_nodes)):
                        similarity_score = self.calculate_tongyi_similarity(
                            new_nodes[i]['content'], new_nodes[j]['content']
                        )

                        if similarity_score > 0.5:
                            distance_weight = 1.0 - similarity_score
                            self.create_similarity_relationship(
                                similarity_session, new_nodes[i], new_nodes[j], similarity_score, distance_weight
                            )
                            similarity_count += 1

                        time.sleep(0.2)

                print(f"âš¡ æ–°å»º {similarity_count} ä¸ªç›¸ä¼¼æ€§å…³ç³»")

            print("âœ… å¢é‡æ›´æ–°å®Œæˆï¼")
            return True

    def process_content_node(self, session, note):
        """å¤„ç†å†…å®¹èŠ‚ç‚¹ - å°†Contentå’ŒDialogåˆå¹¶æ€»ç»“"""
        content = note.get('content', '')
        dialog = self.parse_dialog(note.get('dialog', '[]'))
        note_id = note.get('note_id')

        # ç”ŸæˆContentå’ŒDialogçš„ç»¼åˆæ€»ç»“ï¼ˆ100å­—ï¼‰
        comprehensive_summary = self.summarize_content_with_dialogs(content, dialog)

        # åˆ›å»ºå†…å®¹èŠ‚ç‚¹ï¼Œä½¿ç”¨ç»¼åˆæ€»ç»“ä½œä¸ºä¸»è¦å†…å®¹
        session.run("""
            CREATE (c:Content {
                id: $note_id,
                content: $original_content,
                summary: $comprehensive_summary,
                dialog_count: $dialog_count,
                user_id: $user_id,
                group_id: $group_id,
                position_x: $x_pos,
                position_y: $y_pos,
                deleted: $deleted,
                history_content: $history
            })
        """, note_id=note_id,
                    original_content=content,
                    comprehensive_summary=comprehensive_summary,
                    dialog_count=len(dialog),
                    user_id=note.get('user_id'),
                    group_id=note.get('group_id'),
                    x_pos=note.get('position', {}).get('x', 0),
                    y_pos=note.get('position', {}).get('y', 0),
                    deleted=note.get('deleted', False),
                    history=json.dumps(note.get('history_content', []), ensure_ascii=False))

    def analyze_similarity_with_llm(self, content1_summary: str, content2_summary: str, similarity_score: float) -> str:
        """ä½¿ç”¨LLMåˆ†æä¸¤ä¸ªå†…å®¹ä¹‹é—´çš„ç›¸ä¼¼æ€§å…³ç³»"""
        if not self.client:
            return f"ä¸¤ä¸ªå†…å®¹çš„ç›¸ä¼¼åº¦ä¸º{similarity_score:.3f}ï¼Œå­˜åœ¨ä¸€å®šç¨‹åº¦çš„è¯­ä¹‰å…³è”ã€‚"

        analysis_prompt = f"""
        å†…å®¹1: {content1_summary}
        å†…å®¹2: {content2_summary}
        ç›¸ä¼¼åº¦åˆ†æ•°: {similarity_score:.3f}

        è¯·åˆ†æè¿™ä¸¤ä¸ªå†…å®¹ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œè¯´æ˜å®ƒä»¬åœ¨å“ªäº›æ–¹é¢ç›¸å…³è”ï¼Œä¸ºä»€ä¹ˆä¼šæœ‰è¿™æ ·çš„ç›¸ä¼¼åº¦ã€‚
        """

        response = self.client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {
                    "role": "system",
                    "content": "è¯·åˆ†æä¸¤ä¸ªå†…å®¹ä¹‹é—´çš„ç›¸ä¼¼æ€§å…³ç³»ï¼Œé‡ç‚¹è¯´æ˜å®ƒä»¬çš„å…³è”ç‚¹å’Œç›¸ä¼¼åŸå› ã€‚å›å¤åº”è¯¥æ˜¯ä¸­æ–‡ï¼Œæ§åˆ¶åœ¨50å­—ä»¥å†…ï¼Œè¦å…·ä½“ä¸”æœ‰åˆ†æä»·å€¼ã€‚"
                },
                {
                    "role": "user",
                    "content": analysis_prompt
                }
            ],
            max_tokens=80,
            temperature=0.3
        )

        analysis = response.choices[0].message.content.strip()
        time.sleep(0.1)
        return analysis

    def incremental_export_knowledge_graph(self, output_dir="./output/"):
        """å¢é‡å¯¼å‡ºçŸ¥è¯†å›¾è°±ï¼ˆåªåˆ†ææ–°å¢çš„ç›¸ä¼¼æ€§å…³ç³»ï¼‰"""

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨å¯¼å‡ºæ–‡ä»¶
        main_file = os.path.join(output_dir, 'knowledge_graph_complete.json')
        existing_relationships = set()

        if os.path.exists(main_file):
            try:
                with open(main_file, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
                    for rel in existing_data.get('relationships', []):
                        # ä½¿ç”¨æ’åºåçš„IDå¯¹ä½œä¸ºå…³ç³»çš„å”¯ä¸€æ ‡è¯†
                        id1, id2 = sorted([rel['content1_id'], rel['content2_id']])
                        existing_relationships.add(f"{id1}_{id2}")
                print(f"ğŸ“„ æ£€æµ‹åˆ°å·²æœ‰å¯¼å‡ºæ–‡ä»¶ï¼ŒåŒ…å« {len(existing_relationships)} ä¸ªå·²åˆ†æçš„å…³ç³»")
            except Exception as e:
                print(f"âš ï¸ è¯»å–å·²æœ‰æ–‡ä»¶å¤±è´¥: {e}")
                existing_relationships = set()

        # è·å–æ‰€æœ‰ContentèŠ‚ç‚¹
        content_nodes = self.execute_query("""
            MATCH (c:Content)
            RETURN c.id as id, c.content as content, c.summary as summary,
                   c.dialog_count as dialog_count, c.user_id as user_id,
                   c.group_id as group_id, c.position_x as position_x,
                   c.position_y as position_y, c.deleted as deleted,
                   c.history_content as history_content
            ORDER BY c.id
        """)

        # è·å–æ‰€æœ‰ç›¸ä¼¼æ€§å…³ç³»
        all_relationships = self.execute_query("""
            MATCH (c1:Content)-[r:SIMILAR_CONTENT]-(c2:Content)
            WHERE c1.id < c2.id  // é¿å…é‡å¤
            RETURN c1.id as content1_id, c1.summary as content1_summary,
                   c2.id as content2_id, c2.summary as content2_summary,
                   r.similarity as similarity, r.weight as weight,
                   r.distance as distance, r.type as type, r.method as method
            ORDER BY r.similarity DESC
        """)

        # è¯†åˆ«æ–°å¢çš„å…³ç³»
        new_relationships = []
        existing_analyzed_relationships = []

        for rel in all_relationships:
            id1, id2 = sorted([rel['content1_id'], rel['content2_id']])
            rel_key = f"{id1}_{id2}"

            if rel_key in existing_relationships:
                # è¿™æ˜¯å·²ç»åˆ†æè¿‡çš„å…³ç³»ï¼Œéœ€è¦ä¿ç•™åŸæœ‰åˆ†æ
                existing_analyzed_relationships.append(rel)
            else:
                # è¿™æ˜¯æ–°çš„å…³ç³»ï¼Œéœ€è¦è¿›è¡ŒLLMåˆ†æ
                new_relationships.append(rel)

        if not new_relationships:
            print("ğŸ“­ æ²¡æœ‰æ–°çš„ç›¸ä¼¼æ€§å…³ç³»éœ€è¦åˆ†æ")
            return

        print(f"ğŸ†• å‘ç° {len(new_relationships)} ä¸ªæ–°çš„ç›¸ä¼¼æ€§å…³ç³»éœ€è¦åˆ†æ...")
        print(f"ğŸ“‹ ä¿ç•™ {len(existing_analyzed_relationships)} ä¸ªå·²åˆ†æçš„å…³ç³»")

        # åªä¸ºæ–°å…³ç³»ç”ŸæˆLLMåˆ†æ
        enhanced_new_relationships = []
        for i, rel in enumerate(new_relationships):
            print(f"åˆ†æè¿›åº¦: {i + 1}/{len(new_relationships)}")

            llm_analysis = self.analyze_similarity_with_llm(
                rel['content1_summary'],
                rel['content2_summary'],
                rel['similarity']
            )

            enhanced_rel = {
                'content1_id': rel['content1_id'],
                'content2_id': rel['content2_id'],
                'similarity': rel['similarity'],
                'weight': rel['weight'],
                'distance': rel['distance'],
                'type': rel['type'],
                'method': rel['method'],
                'llm_analysis': llm_analysis,
                'content1_summary': rel['content1_summary'],
                'content2_summary': rel['content2_summary']
            }
            enhanced_new_relationships.append(enhanced_rel)

        # å¦‚æœæœ‰å·²å­˜åœ¨çš„åˆ†æç»“æœï¼Œéœ€è¦è¯»å–å¹¶åˆå¹¶
        all_enhanced_relationships = []

        if existing_relationships and os.path.exists(main_file):
            try:
                with open(main_file, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
                    all_enhanced_relationships = existing_data.get('relationships', [])
                print(f"ğŸ“‹ åŠ è½½äº† {len(all_enhanced_relationships)} ä¸ªå·²æœ‰åˆ†æç»“æœ")
            except Exception as e:
                print(f"âš ï¸ åŠ è½½å·²æœ‰åˆ†æç»“æœå¤±è´¥: {e}")

        # æ·»åŠ æ–°åˆ†æçš„å…³ç³»
        all_enhanced_relationships.extend(enhanced_new_relationships)

        # æ„å»ºå®Œæ•´çš„å›¾è°±æ•°æ®ç»“æ„
        knowledge_graph = {
            'nodes': content_nodes,
            'relationships': all_enhanced_relationships
        }

        # å¯¼å‡ºä¸»æ–‡ä»¶
        with open(main_file, 'w', encoding='utf-8') as f:
            json.dump(knowledge_graph, f, ensure_ascii=False, indent=2)

        # å¯¼å‡ºèŠ‚ç‚¹æ–‡ä»¶
        nodes_file = os.path.join(output_dir, 'nodes.json')
        with open(nodes_file, 'w', encoding='utf-8') as f:
            json.dump(content_nodes, f, ensure_ascii=False, indent=2)

        # å¯¼å‡ºå…³ç³»æ–‡ä»¶
        relationships_file = os.path.join(output_dir, 'relationships.json')
        with open(relationships_file, 'w', encoding='utf-8') as f:
            json.dump(all_enhanced_relationships, f, ensure_ascii=False, indent=2)

        print(f"\nâœ… å¢é‡å¯¼å‡ºå®Œæˆï¼")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        print(
            f"ğŸ“„ å®Œæ•´ç‰ˆæœ¬: knowledge_graph_complete.json ({len(content_nodes)} èŠ‚ç‚¹, {len(all_enhanced_relationships)} å…³ç³»)")
        print(f"ğŸ†• æ–°å¢åˆ†æ: {len(enhanced_new_relationships)} ä¸ªå…³ç³»")
        print(f"ğŸ“„ èŠ‚ç‚¹æ–‡ä»¶: nodes.json")
        print(f"ğŸ“„ å…³ç³»æ–‡ä»¶: relationships.json")


def main():
    NEO4J_URI = "neo4j+s://9cf25794.databases.neo4j.io"
    NEO4J_USERNAME = "neo4j"
    NEO4J_PASSWORD = "NRuAyaDCUhFxXImdnWHBszPdyW5Ux6MHDBl9jwnOj5g"
    OPENAI_API_KEY = "sk-guFVSLItjWTGMkkeSjwKTFQqSa23H5lsLqbYdHZqBboz6k6D"
    TONGYI_API_KEY = ("sk-3160bc2566fe45ba9348a789c5316bbc")

    kg = ContentFocusedKG(NEO4J_URI, NEO4J_USERNAME, NEO4J_PASSWORD, OPENAI_API_KEY, TONGYI_API_KEY)

    # æ™ºèƒ½æ›´æ–°ä½¿ç”¨æ–¹å¼ï¼š
    # 1. è‡ªåŠ¨åˆ¤æ–­æ¨¡å¼ï¼ˆæ¨èï¼‰- æ•°æ®åº“ä¸ºç©ºæ—¶å…¨é‡å¯¼å…¥ï¼Œå¦åˆ™å¢é‡æ›´æ–°
    has_updates = kg.update_graph('./data/sample.json')

    # 2. å¼ºåˆ¶å…¨é‡é‡å»º
    # has_updates = kg.update_graph('./data/sample.json', force_full_rebuild=True)

    # 3. å¢é‡å¯¼å‡ºçŸ¥è¯†å›¾è°±ï¼ˆåªåœ¨æœ‰æ›´æ–°æ—¶è¿›è¡Œå¯¼å‡ºï¼Œä¸”åªåˆ†ææ–°å¢çš„å…³ç³»ï¼‰
    if has_updates:
        print("ğŸ”„ å¼€å§‹å¢é‡å¯¼å‡º...")
        kg.incremental_export_knowledge_graph("./knowledge_graph_output/")
    else:
        print("â­ï¸ æ— æ›´æ–°å†…å®¹ï¼Œè·³è¿‡å¯¼å‡º")

    kg.close()


if __name__ == "__main__":
    main()